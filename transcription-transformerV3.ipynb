{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNcZsa52tEu5pDjFDtjpEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zeerroth/transcript-transformer/blob/main/transcription-transformerV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du_FiC_PPRN5",
        "outputId": "efd4a164-9c5e-4d69-8e54-9fa15a0b188c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.27.0\n",
            "  Downloading openai-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (3.11.10)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (2.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading openai-0.27.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, openai, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 openai-0.27.0 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.27.0 gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import gradio as gr\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "alx8Q-0OPR2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely input your OpenAI API key\n",
        "openai.api_key = getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrbiPMDFPTE7",
        "outputId": "704ff6e6-a41a-4c83-b35f-7b1d61e7671f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_file_size_for_single_chunk(file_content, max_input_tokens=119000):\n",
        "    \"\"\"\n",
        "    Validates the size of the input file content and ensures it doesn't exceed the token limit for single chunk processing.\n",
        "    \"\"\"\n",
        "    if not file_content:\n",
        "        raise ValueError(\"File content is empty. Please upload a valid file.\")\n",
        "\n",
        "    # Decode and check length\n",
        "    text = file_content.decode(\"utf-8\")\n",
        "    token_count = len(text.split())\n",
        "\n",
        "    if token_count > max_input_tokens:\n",
        "        raise ValueError(f\"Input exceeds the maximum token limit of {max_input_tokens}. Please upload a smaller file.\")\n",
        "\n",
        "    print(f\"File validated with {token_count} tokens.\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "QryaY5nbPUtO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_context(context, max_tokens=100000):\n",
        "    \"\"\"\n",
        "    Truncate the context while retaining the most relevant input and outputs.\n",
        "    Prioritizes the system prompt, the current chunk, and the latest response summaries.\n",
        "    \"\"\"\n",
        "    total_tokens = sum(len(msg[\"content\"]) for msg in context)\n",
        "    while total_tokens > max_tokens:\n",
        "        # Remove the oldest assistant response or user input, keeping system prompt and latest context\n",
        "        if len(context) > 2:\n",
        "            context.pop(1)\n",
        "        total_tokens = sum(len(msg[\"content\"]) for msg in context)\n",
        "    return context"
      ],
      "metadata": {
        "id": "altEEGyPPXuG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_single_chunk(text):\n",
        "    \"\"\"\n",
        "    Processes the entire text as a single chunk with an improved system prompt to minimize the need for refinement.\n",
        "    \"\"\"\n",
        "    SYSTEM_PROMPT = f\"\"\"\n",
        "Transform the following transcript into a detailed and comprehensive teaching transcript suitable for a university-level lecture. Your output must:\n",
        "\n",
        "**SPEND AS MUCH AS YOU NEED TO THINK ABOUT HOW TO INCREASE THE LENGTH OF THE RESPONSE AND ENSURE RESPOSE CONTAINS AT LEAST 8000 WORDS**\n",
        "**ANSWWER WITH MAXIMUM VERBOSITY**\n",
        "\n",
        "1. **Length and Depth**:\n",
        "   - Contain a minimum of 8,000 words (~11,000 tokens) and maximum of 12000 words(~16000 tokens).\n",
        "   - Explore each topic in-depth with detailed explanations, real-world examples, case studies, and actionable insights in each section.\n",
        "   - Include additional subtopics and tangentially related concepts to enrich the lecture.\n",
        "\n",
        "2. **Structure**:\n",
        "   - Divide the content into clearly defined sections and subsections, each with appropriate headings.\n",
        "   - End each section with a summary of key points, actionable steps, and reflective questions.\n",
        "   - Add multiple examples at the end of the each section to enchance understanding.\n",
        "\n",
        "3. **Expansion Beyond the Transcript**:\n",
        "   - Use the transcript content as a foundation and incorporate it throughout the material.\n",
        "   - Introduce new perspectives, including historical context, interdisciplinary connections, and current trends in each section.\n",
        "   - Provide hypothetical future scenarios, thought experiments, and potential applications of the concepts discussed in each section.\n",
        "\n",
        "4. **Audience Engagement**:\n",
        "   - Pose rhetorical questions and include practical applications to connect theory with practice in each section.\n",
        "   - Suggest activities or exercises that reinforce the key concepts in each section.\n",
        "   - Design engaging discussions, such as debates on controversial aspects of the topics in each section.\n",
        "\n",
        "5. **New and Enriched Content**:\n",
        "   - Expand sections with examples, analogies, and hypothetical scenarios to illustrate concepts effectively in each section.\n",
        "   - Add perspectives on challenges, criticisms, or alternative approaches to the topics discussed in each section.\n",
        "   - Include industry-specific case studies and compare diverse implementations in each section.\n",
        "\n",
        "6. **Professional Tone and Logical Flow**:\n",
        "   - Maintain a logical progression of ideas, ensuring smooth transitions between sections.\n",
        "\n",
        "The lecture should be self-contained, go beyond the input transcript, and provide a rich and engaging learning experience with adequate depth and breadth to fulfill the word count requirement.\n",
        "**SPEND AS MUCH AS YOU NEED TO THINK ABOUT HOW TO INCREASE THE LENGTH OF THE RESPONSE AND ENSURE RESPOSE CONTAINS AT LEAST 8000 WORDS**\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    context = [{\"role\": \"user\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": text}]\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI API to generate the initial output\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"o1-mini\",\n",
        "            messages=context,\n",
        "            temperature=1.0,\n",
        "        )\n",
        "\n",
        "        # Extract the response content\n",
        "        transformed_transcript = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        word_count = len(transformed_transcript.split())\n",
        "\n",
        "        print(f\"Initial output word count: {word_count}\")\n",
        "        return transformed_transcript\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing single chunk: {e}\""
      ],
      "metadata": {
        "id": "xGCHBIztPaG9"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_transcript_single_chunk(file_content):\n",
        "    \"\"\"\n",
        "    Processes the transcript as a single chunk for better continuity and relevance.\n",
        "    Ensures the final output meets the minimum length requirements of 8,000 words.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate file size\n",
        "        if not file_content:\n",
        "            raise ValueError(\"No file content provided. Please upload a valid file.\")\n",
        "\n",
        "        text = validate_file_size_for_single_chunk(file_content, max_input_tokens=119000)\n",
        "\n",
        "        # Step 2: Transform the text as a single chunk\n",
        "        transformed_transcript = transform_single_chunk(text)\n",
        "\n",
        "        # Step 3: Return the transformed transcript\n",
        "        return transformed_transcript\n",
        "\n",
        "    except ValueError as e:\n",
        "        return str(e)  # Return error message if the file size is invalid\n",
        "    except Exception as e:\n",
        "        return f\"Error processing transcript: {e}\""
      ],
      "metadata": {
        "id": "EvIqAUDvPbtH"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_single_chunk_interface(file):\n",
        "    if not file:\n",
        "        return \"No file uploaded. Please upload a valid transcript file.\"\n",
        "    return process_transcript_single_chunk(file)\n"
      ],
      "metadata": {
        "id": "yY2PJ_3YPd_a"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=gradio_single_chunk_interface,\n",
        "    inputs=gr.File(label=\"Upload Transcript (.txt)\", type=\"binary\"),\n",
        "    outputs=gr.Textbox(label=\"Generated Teaching Transcript\"),\n",
        "    title=\"Generative Transcript Transformer (Single Chunk)\",\n",
        "    description=\"Upload an unstructured transcript to transform it into a structured teaching transcript suitable for a lecture.\",\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "LrOyKdEiPf9Y",
        "outputId": "10e0d3b3-6860-465b-8bf4-490fdd1b8125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://68deaacf6bd9787f04.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://68deaacf6bd9787f04.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File validated with 6398 tokens.\n",
            "Initial output word count: 9225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6__JlGiPlEj"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}